{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting, log in to SQL as the postgres user and run this in the sm database:\n",
    "#  CREATE SCHEMA graphql;\n",
    "#  CREATE EXTENSION \"uuid-ossp\";\n",
    "#  ALTER SCHEMA graphql OWNER TO sm;\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from sm.engine.db import DB\n",
    "from sm.engine.es_export import ESExporter, init_es_conn\n",
    "from sm.engine.util import SMConfig\n",
    "from sm.engine.dataset import Dataset\n",
    "from elasticsearch.client import IndicesClient, IngestClient\n",
    "SMConfig.set_path('../conf/config.json')\n",
    "sm_config = SMConfig.get_conf()\n",
    "db = DB(sm_config['db'])\n",
    "# es = ESExporter(db)\n",
    "# es_conn = init_es_conn(sm_config['elasticsearch'])\n",
    "# es_index = sm_config['elasticsearch']['index']\n",
    "# ingest = IngestClient(es_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitter_data_file = \"r1_0_20180917_submitter_data.csv\"\n",
    "group_membership_file = \"r1_0_20180917_group_membership.csv\"\n",
    "\n",
    "group_keys = ['institution', 'email', 'name', 'pi_email', 'pi_name']\n",
    "dirty_group_keys = ['current_' + key for key in group_keys]\n",
    "clean_group_keys = ['new_' + key for key in group_keys]\n",
    "\n",
    "SubmitterInfo = namedtuple('SubmitterInfo', group_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all institutions, submitters and PIs (now referred to as \"submitter data\")\n",
    "def get_src_datasets():\n",
    "    def normalize_name(person):\n",
    "        if person.get('Name'):\n",
    "            return person['Name']\n",
    "        return (person.get('First_Name', '').strip() + ' ' + person.get('Surname', '').strip()).strip()\n",
    "\n",
    "    src_datasets = []\n",
    "    for id, is_public, metadata in db.select(\"select id, is_public, metadata from dataset\", None):\n",
    "        submitted_by = metadata.get('Submitted_By')\n",
    "        if submitted_by and submitted_by.get('Submitter'):\n",
    "            submitter = submitted_by['Submitter']\n",
    "            pi = submitted_by.get('Principal_Investigator', {})\n",
    "            src_datasets.append({\n",
    "                \"id\": id,\n",
    "                \"is_public\": is_public,\n",
    "                \"institution\": submitted_by.get('Institution', ''),\n",
    "                \"email\": submitter.get('Email', '').lower(),\n",
    "                \"name\": normalize_name(submitter),\n",
    "                \"pi_email\": pi.get('Email', '').lower(),\n",
    "                \"pi_name\": normalize_name(pi),\n",
    "            })\n",
    "    return src_datasets\n",
    "\n",
    "src_datasets = get_src_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump submitter data to file for manual cleaning\n",
    "def dump_submitters_for_manual_cleaning(filename, src_datasets):\n",
    "    groups = (pd.DataFrame(src_datasets)\n",
    "        .sort_values(group_keys)\n",
    "        .groupby(group_keys))\n",
    "\n",
    "    rows = []\n",
    "    for groupkey, group in groups:\n",
    "        rows.append(dict(\n",
    "            [('datasets', group.size)] +\n",
    "            [('earliest', group['id'].min())] +\n",
    "            [('latest', group['id'].max())] +\n",
    "            list(zip(dirty_group_keys, groupkey)) +\n",
    "            list(zip(clean_group_keys, groupkey))\n",
    "        ))\n",
    "        \n",
    "    df = pd.DataFrame(rows)\n",
    "    # Reorder columns & export\n",
    "    df = [['datasets', 'earliest', 'latest', *dirty_group_keys, *clean_group_keys]].to_csv(filename, index=False)\n",
    "\n",
    "if not os.path.isfile(submitter_data_file):\n",
    "    dump_submitters_for_manual_cleaning(submitter_data_file, src_datasets)\n",
    "# Now manually edit the CSV, leaving the \"current_\" columns as-is and cleaning the values in the \"new_\" columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned CSV, unpack it and validate that it is consistent\n",
    "    \n",
    "def read_cleaned_submitters(submitter_file):\n",
    "    combined_cleaned_data = pd.read_csv(submitter_data_file).fillna('')\n",
    "    dirty_data = combined_cleaned_data.loc[:, dirty_group_keys].rename(columns=dict(zip(dirty_group_keys, group_keys)))\n",
    "    clean_data = combined_cleaned_data.loc[:, clean_group_keys].rename(columns=dict(zip(clean_group_keys, group_keys)))\n",
    "    \n",
    "    groups_to_add = set(institution for (institution,) in clean_data[['institution']].values if institution)\n",
    "    submitters_to_add = set((email, name) for email, name in clean_data[['email','name']].values)\n",
    "    pis_to_add = set((email, name) for email, name in clean_data[['pi_email','pi_name']].values if email and name)\n",
    "    users_to_add = submitters_to_add.union(pis_to_add)\n",
    "    \n",
    "    dirty_to_clean = dict(zip(\n",
    "        (SubmitterInfo(*args) for args in dirty_data.values), \n",
    "        (SubmitterInfo(*args) for args in clean_data.values)))\n",
    "    \n",
    "    return dirty_data, clean_data, dirty_to_clean, groups_to_add, users_to_add\n",
    "\n",
    "def validate_cleaned_submitters(src_datasets, clean_data, dirty_to_clean, users_to_add):\n",
    "    for email, df in pd.DataFrame(list(users_to_add), columns=[\"email\",\"name\"]).groupby([\"email\"], as_index=False):\n",
    "        if len(df['name'].values) > 1:\n",
    "            print(f'Submitter/PI email mapped to multiple names: {email} -> {\", \".join(df[\"name\"].values)}')\n",
    "\n",
    "    for idx, series in clean_data.iterrows():\n",
    "        inst, email, name = series.loc[['institution', 'pi_email', 'pi_name']]\n",
    "        if not inst and not (email and name):\n",
    "            print(f'Row {idx+1} has no institution or PI')\n",
    "\n",
    "    for ds in src_datasets:\n",
    "        key = SubmitterInfo(*(ds[key] for key in group_keys))\n",
    "        if not dirty_to_clean.get(key):\n",
    "            print(f'Missing cleaned data for {key}')\n",
    "            \n",
    "dirty_data, clean_data, dirty_to_clean, groups_to_add, users_to_add = read_cleaned_submitters(submitter_data_file)\n",
    "validate_cleaned_submitters(src_datasets, clean_data, dirty_to_clean, users_to_add)\n",
    "# Fix validation errors & rerun if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new groups & users\n",
    "for group in groups_to_add:\n",
    "    if not db.select_one(\"SELECT 1 FROM graphql.group WHERE name = %s\", [group]):\n",
    "        db.alter(\"INSERT INTO graphql.group (name, short_name) VALUES (%s, %s)\", [group, group])\n",
    "    \n",
    "for email, name in users_to_add:\n",
    "    if not db.select_one(\"SELECT 1 FROM graphql.user WHERE LOWER(email) = LOWER(%s)\", [email]):\n",
    "        db.alter(\"\"\"\n",
    "            WITH cred AS (INSERT INTO graphql.credentials (\"emailVerified\") VALUES (FALSE) RETURNING id)\n",
    "            INSERT INTO graphql.user (email, name, role, credentials_id)\n",
    "            SELECT %s, %s, 'user', cred.id\n",
    "            FROM cred;\n",
    "        \"\"\", [email, name])\n",
    "    else:\n",
    "        db.alter('UPDATE graphql.user SET name = %s WHERE email = %s', [name, email])\n",
    "\n",
    "new_group_name_to_id = dict(db.select(\"SELECT name, id FROM graphql.group\"))\n",
    "new_user_email_to_id = dict(db.select(\"SELECT email, id FROM graphql.user\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update datasets\n",
    "def update_dataset(ds, dirty_to_clean, new_group_name_to_id, new_user_email_to_id):\n",
    "    ds_id = ds['id']\n",
    "    cleaned = dirty_to_clean[SubmitterInfo(*(ds[key] for key in group_keys))]\n",
    "    user_id = new_user_email_to_id[cleaned.email]\n",
    "    group_id = cleaned.institution and new_group_name_to_id[cleaned.institution] or None\n",
    "    metadata, = db.select_one('SELECT metadata FROM dataset WHERE id = %s', [ds_id])\n",
    "    \n",
    "    if not db.select_one('SELECT 1 FROM graphql.dataset WHERE id = %s', [ds_id]):\n",
    "        db.alter(\"INSERT INTO graphql.dataset (id, user_id, group_id) VALUES (%s, %s, %s)\", [ds_id, user_id, group_id])\n",
    "    else:\n",
    "        db.alter(\"UPDATE graphql.dataset SET user_id = %s, group_id = %s WHERE id = %s\", [user_id, group_id, ds_id])\n",
    "       \n",
    "    metadata['Submitted_By'] = {\n",
    "        # TODO: Remove Institution & Submitter once code is migrated\n",
    "        'Institution': cleaned.institution,\n",
    "        'Submitter': {\"Email\": cleaned.email, \"Name\": cleaned.name},\n",
    "        'Principal_Investigator': {\"Email\": cleaned.pi_email, \"Name\": cleaned.pi_name} if cleaned.pi_email else None,\n",
    "    }\n",
    "    db.alter('UPDATE dataset SET metadata = %s WHERE id = %s', [json.dumps(metadata), ds_id])\n",
    "    \n",
    "    # TODO: merge PR #51 so that this works:\n",
    "#     es.update_ds(ds_id, ['submitter_id', 'group_id', 'metadata'])\n",
    "    \n",
    "for ds in src_datasets:\n",
    "    update_dataset(ds, dirty_to_clean, new_group_name_to_id, new_user_email_to_id)\n",
    "    \n",
    "# Update src_datasets with cleaned data, as it's used for statistics in the membership queries\n",
    "src_datasets = get_src_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove fields from elasticsearch (NOTE: The mappings remain in Elasticsearch until the index is recreated, because ES no longer supports deleting mappings)\n",
    "def remove_fields_from_elasticsearch():\n",
    "    fields_to_remove = [\n",
    "        'ds_meta.Submitted_By.Institution', \n",
    "        'ds_meta.Submitted_By.Submitter.Email',\n",
    "        'ds_meta.Submitted_By.Submitter.First_Name',\n",
    "        'ds_meta.Submitted_By.Submitter.Surname',\n",
    "        'ds_meta.Submitted_By.Submitter',\n",
    "        'ds_meta.Submitted_By.Principal_Investigator.First_Name',\n",
    "        'ds_meta.Submitted_By.Principal_Investigator.Surname'\n",
    "    ]\n",
    "    pipeline_id = 'remove_old_fields'\n",
    "    ingest.put_pipeline(\n",
    "        id=pipeline_id,\n",
    "        body={'processors': \n",
    "              [{'remove': {'field': field, 'ignore_failure': True}} for field in fields_to_remove]\n",
    "             })\n",
    "\n",
    "    es_conn.update_by_query(\n",
    "        index=es_index,\n",
    "        body={'query': {}},\n",
    "        params={'pipeline': pipeline_id, 'wait_for_completion': True})\n",
    "\n",
    "    ingest.delete_pipeline(id=pipeline_id)\n",
    "    \n",
    "remove_fields_from_elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump calculated group memberships for manual checking\n",
    "def dump_group_membership_for_manual_cleaning(filename, clean_data):\n",
    "    src_datasets_df = pd.DataFrame(src_datasets)\n",
    "    institutions_and_emails = np.concatenate([\n",
    "        clean_data[['institution','email','name']].values,\n",
    "        clean_data[['institution','pi_email','pi_name']].values])\n",
    "\n",
    "    columns = ['group', 'email', 'name', 'datasets submitted', 'datasets as PI', 'datasets as PI to someone else', 'role']\n",
    "    users_in_groups = []\n",
    "    for inst, email, name in pd.unique([(inst, email, name) for inst, email, name in institutions_and_emails]):\n",
    "        if inst and email:\n",
    "            df_inst = src_datasets_df['institution'] == inst\n",
    "            df_subm = src_datasets_df['email'] == email\n",
    "            df_not_subm = src_datasets_df['email'] != email\n",
    "            df_pi = src_datasets_df['pi_email'] == email\n",
    "            datasets_as_submitter = src_datasets_df[df_inst & df_subm].shape[0]\n",
    "            datasets_as_pi = src_datasets_df[df_inst & df_pi].shape[0]\n",
    "            datasets_as_pi_to_someone_else = src_datasets_df[df_inst & df_pi & df_not_subm].shape[0]\n",
    "            role = 'PRINCIPAL_INVESTIGATOR' if datasets_as_pi > 0 else 'MEMBER'\n",
    "            users_in_groups.append([inst, email, name, datasets_as_submitter, datasets_as_pi, datasets_as_pi_to_someone_else, role])\n",
    "        \n",
    "    df = pd.DataFrame(users_in_groups, columns=columns).sort_values(columns)\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "    \n",
    "if not os.path.isfile(group_membership_file):\n",
    "    dump_group_membership_for_manual_cleaning(group_membership_file, clean_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and validate group memberships\n",
    "    \n",
    "def read_cleaned_group_membership(filename):\n",
    "    data = pd.read_csv(filename).fillna('')\n",
    "    data = data[['group', 'email', 'role']]\n",
    "    \n",
    "    #validate\n",
    "    valid_data = []\n",
    "    for idx, (group, email, role) in enumerate(data.values):\n",
    "        if not new_group_name_to_id.get(group):\n",
    "            print(f\"Unrecognized group {group} on line {idx}\")\n",
    "        elif not new_user_email_to_id.get(email):\n",
    "            print(f\"Unrecognized user {email} on line {idx}\")\n",
    "        elif not role in ['PRINCIPAL_INVESTIGATOR','MEMBER', '']:\n",
    "            print(f\"Unrecognized role {role} on line {idx}\")\n",
    "        elif role != '':\n",
    "            valid_data.append((group, email, role))\n",
    "            \n",
    "    return valid_data\n",
    "            \n",
    "group_membership = read_cleaned_group_membership(group_membership_file)\n",
    "\n",
    "# Fix data and rerun this cell if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, email, role in group_membership:\n",
    "    group_id = new_group_name_to_id[group]\n",
    "    user_id = new_user_email_to_id[email]\n",
    "    if not db.select_one('SELECT 1 FROM graphql.user_group WHERE group_id = %s AND user_id = %s', [group_id, user_id]):\n",
    "        db.alter(\"INSERT INTO graphql.user_group (group_id, user_id, role) VALUES (%s, %s, %s)\", [group_id, user_id, role])\n",
    "    else:\n",
    "        db.alter(\"UPDATE graphql.user_group SET role = %s WHERE group_id = %s AND user_id = %s\", [role, group_id, user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sm]",
   "language": "python",
   "name": "conda-env-sm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
